{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-2NB8ZQ5UyAa"
      },
      "source": [
        "### Импорт"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QNp-zNh1UnRz"
      },
      "outputs": [],
      "source": [
        "!python3 -m spacy download en_core_web_sm"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "Kcn0zKQFUrlH"
      },
      "outputs": [],
      "source": [
        "import json\n",
        "import spacy\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import gensim.models\n",
        "\n",
        "from zipfile import ZipFile\n",
        "from sklearn.metrics import classification_report\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "random_state=9"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "iYLTW1vGSZKJ"
      },
      "outputs": [],
      "source": [
        "!mkdir ~/.kaggle\n",
        "!touch ~/.kaggle/kaggle.json"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "j9XIaPOUQ5t1"
      },
      "outputs": [],
      "source": [
        "api_token = {\"username\":\"w1nston\",\"key\":\"81bfe28fc3d2a7574476ab1222e695db\"}\n",
        "\n",
        "with open('/root/.kaggle/kaggle.json', 'w+') as file:\n",
        "    json.dump(api_token, file)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "yTIuhvdCQ733"
      },
      "outputs": [],
      "source": [
        "!chmod 600 ~/.kaggle/kaggle.json"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "HumGwcqdQ97N",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e261e116-301d-44d8-ccab-7bdea530000e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading spam-or-not-spam-dataset.zip to /content\n",
            "\r  0% 0.00/1.16M [00:00<?, ?B/s]\n",
            "\r100% 1.16M/1.16M [00:00<00:00, 54.5MB/s]\n"
          ]
        }
      ],
      "source": [
        "!kaggle datasets download -d ozlerhakan/spam-or-not-spam-dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "l_fEJVZNVury"
      },
      "outputs": [],
      "source": [
        "with ZipFile('spam-or-not-spam-dataset.zip', 'r') as zip_ref:\n",
        "    zip_ref.extractall()"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Предобработка"
      ],
      "metadata": {
        "id": "OngInPWUF9H8"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "jjoaiqu7VbgP"
      },
      "outputs": [],
      "source": [
        "data = pd.read_csv('spam_or_not_spam.csv', encoding='iso-8859-1').rename(columns={'email': 'text'})"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "8RCOnoaUCFDt"
      },
      "outputs": [],
      "source": [
        "data = data[['text', 'label']].dropna()"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "nlp = spacy.load(\"en_core_web_sm\")"
      ],
      "metadata": {
        "id": "C0IelcJjY35E"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data['cleaned_text'] = data['text'].apply(\n",
        "    lambda x: ' '.join(\n",
        "        token.lemma_.lower() for token in nlp(x) if\n",
        "        not token.is_stop\n",
        "        and not token.is_punct\n",
        "        and not token.is_digit\n",
        "        and not token.like_email\n",
        "        and not token.like_num\n",
        "        and not token.is_space\n",
        "    )\n",
        ")"
      ],
      "metadata": {
        "id": "hJl-7N8C8cwW"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "hdZnzUE_VMev"
      },
      "outputs": [],
      "source": [
        "X_train, X_test, y_train, y_test = train_test_split(data['cleaned_text'], data['label'],\n",
        "                                                    random_state=random_state, shuffle = True)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "sentences = [elem.split() for elem in data['cleaned_text']]"
      ],
      "metadata": {
        "id": "krRJz6R59gAK"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### SG, CBOW, FastText сравнение"
      ],
      "metadata": {
        "id": "tglHt9QbDMBR"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "4KnaDUF2cPNo"
      },
      "outputs": [],
      "source": [
        "# SG\n",
        "sg_model = gensim.models.Word2Vec(\n",
        "    sentences=sentences,\n",
        "    vector_size=256,\n",
        "    window=7,\n",
        "    min_count=10,\n",
        "    sg=1,\n",
        "    hs=0,\n",
        "    negative=5,\n",
        "    epochs=25,\n",
        "    seed=random_state,\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "ySO_7saPcPyJ"
      },
      "outputs": [],
      "source": [
        "# CBOW\n",
        "cbow_model = gensim.models.Word2Vec(\n",
        "    sentences=sentences,\n",
        "    vector_size=256,\n",
        "    window=7,\n",
        "    min_count=10,\n",
        "    sg=0,\n",
        "    hs=0,\n",
        "    negative=5,\n",
        "    epochs=25,\n",
        "    seed=random_state,\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "id": "HFDJ9sGNf9eF"
      },
      "outputs": [],
      "source": [
        "# FastText\n",
        "gensim.models.fasttext.FastText\n",
        "ft_model = gensim.models.Word2Vec(\n",
        "    sentences=sentences,\n",
        "    vector_size=256,\n",
        "    window=7,\n",
        "    min_count=10,\n",
        "    sg=0,\n",
        "    hs=0,\n",
        "    negative=5,\n",
        "    epochs=25,\n",
        "    seed=random_state,\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "sg_model.wv.most_similar(positive=['tree'], topn=5)"
      ],
      "metadata": {
        "id": "UGY_CH0TEne_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "cbow_model.wv.most_similar(positive=['tree'], topn=5)"
      ],
      "metadata": {
        "id": "7lNVPnIMFJI1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "ft_model.wv.most_similar(positive=['tree'], topn=5)"
      ],
      "metadata": {
        "id": "GWaoWRDdFMsg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(sg_model.wv.doesnt_match(['january', 'february', 'march', 'tree']))\n",
        "print(cbow_model.wv.doesnt_match(['january', 'february', 'march', 'tree']))\n",
        "print(ft_model.wv.doesnt_match(['january', 'february', 'march', 'tree']))"
      ],
      "metadata": {
        "id": "PJj5XUdcFcT7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Обучение LogisticRegression"
      ],
      "metadata": {
        "id": "eylsrkZrDiae"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### SG"
      ],
      "metadata": {
        "id": "sCpusJHCgWpA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def text_to_vector(text, model):\n",
        "    word_vectors = model.wv\n",
        "    text_vec = []\n",
        "    for word in text.split():\n",
        "        try:\n",
        "          text_vec.append(word_vectors[word])\n",
        "        except KeyError: #обработка Out of Vocabulary\n",
        "          text_vec.append([0 for i in range(256)])\n",
        "\n",
        "    if len(text_vec) == 0: #для пустых текстов после предобработки\n",
        "        text_vec.append([0 for i in range(256)])\n",
        "\n",
        "    text_vec_out = np.mean(text_vec, axis=0)\n",
        "    return text_vec_out"
      ],
      "metadata": {
        "id": "NtALauoeaO2E"
      },
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X_train_vectorized = [text_to_vector(text, sg_model) for text in X_train]\n",
        "X_test_vectorized = [text_to_vector(text, sg_model) for text in X_test]\n"
      ],
      "metadata": {
        "id": "MQr5nTl3dSs_"
      },
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for i, elem in enumerate(X_train_vectorized):\n",
        "  if len(elem) != 256:\n",
        "    print(len(elem))\n",
        "    #print(f'elem {i} removed. Len: {elem.shape}')\n",
        ""
      ],
      "metadata": {
        "id": "aU-AXycD_lVs"
      },
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "id": "-963KHu6Ty3A",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b6ff320d-8070-4953-f7e2-f1e1b64bb790"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.98      0.99      0.99       616\n",
            "           1       0.97      0.90      0.93       134\n",
            "\n",
            "    accuracy                           0.98       750\n",
            "   macro avg       0.97      0.94      0.96       750\n",
            "weighted avg       0.98      0.98      0.98       750\n",
            "\n"
          ]
        }
      ],
      "source": [
        "logreg = LogisticRegression().fit(X_train_vectorized, y_train)\n",
        "preds = logreg.predict(X_test_vectorized)\n",
        "print(classification_report(y_test, preds))"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### CBOW"
      ],
      "metadata": {
        "id": "XvZIba6Egbv_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def text_to_vector(text, model):\n",
        "    word_vectors = model.wv\n",
        "    text_vec = []\n",
        "    for word in text.split():\n",
        "        try:\n",
        "          text_vec.append(word_vectors[word])\n",
        "        except KeyError: #обработка Out of Vocabulary\n",
        "          text_vec.append([0 for i in range(256)])\n",
        "\n",
        "    if len(text_vec) == 0: #для пустых текстов после предобработки\n",
        "        text_vec.append([0 for i in range(256)])\n",
        "\n",
        "    text_vec_out = np.mean(text_vec, axis=0)\n",
        "    return text_vec_out"
      ],
      "metadata": {
        "id": "Mw04LXFDGF_3"
      },
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X_train_vectorized = [text_to_vector(text, cbow_model) for text in X_train]\n",
        "X_test_vectorized = [text_to_vector(text, cbow_model) for text in X_test]\n"
      ],
      "metadata": {
        "id": "IJ107R__GGAI"
      },
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for i, elem in enumerate(X_train_vectorized):\n",
        "  if len(elem) != 256:\n",
        "    print(len(elem))\n",
        "    #print(f'elem {i} removed. Len: {elem.shape}')\n"
      ],
      "metadata": {
        "id": "aoSzs_Y6GGAK"
      },
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "25cad284-7423-4bab-ca6e-42ecec24d978",
        "id": "ffEL2vctGGAL"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.99      0.99      0.99       616\n",
            "           1       0.94      0.96      0.95       134\n",
            "\n",
            "    accuracy                           0.98       750\n",
            "   macro avg       0.97      0.97      0.97       750\n",
            "weighted avg       0.98      0.98      0.98       750\n",
            "\n"
          ]
        }
      ],
      "source": [
        "logreg = LogisticRegression().fit(X_train_vectorized, y_train)\n",
        "preds = logreg.predict(X_test_vectorized)\n",
        "print(classification_report(y_test, preds))"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### FastText"
      ],
      "metadata": {
        "id": "rv1YW7O0gdNt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def text_to_vector(text, model):\n",
        "    word_vectors = model.wv\n",
        "    text_vec = []\n",
        "    for word in text.split():\n",
        "        try:\n",
        "          text_vec.append(word_vectors[word])\n",
        "        except KeyError: #обработка Out of Vocabulary\n",
        "          text_vec.append([0 for i in range(256)])\n",
        "\n",
        "    if len(text_vec) == 0: #для пустых текстов после предобработки\n",
        "        text_vec.append([0 for i in range(256)])\n",
        "\n",
        "    text_vec_out = np.mean(text_vec, axis=0)\n",
        "    return text_vec_out"
      ],
      "metadata": {
        "id": "H6RQS3jeGHYh"
      },
      "execution_count": 29,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X_train_vectorized = [text_to_vector(text, ft_model) for text in X_train]\n",
        "X_test_vectorized = [text_to_vector(text, ft_model) for text in X_test]\n"
      ],
      "metadata": {
        "id": "rW24NvB5GHYw"
      },
      "execution_count": 30,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "84ccc160-552d-4cb4-e06a-482d8a12fa45",
        "id": "K0DfPLexGHYx"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.99      0.99      0.99       616\n",
            "           1       0.95      0.96      0.95       134\n",
            "\n",
            "    accuracy                           0.98       750\n",
            "   macro avg       0.97      0.97      0.97       750\n",
            "weighted avg       0.98      0.98      0.98       750\n",
            "\n"
          ]
        }
      ],
      "source": [
        "logreg = LogisticRegression().fit(X_train_vectorized, y_train)\n",
        "preds = logreg.predict(X_test_vectorized)\n",
        "print(classification_report(y_test, preds))"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "TPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}