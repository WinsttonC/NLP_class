{"cells":[{"cell_type":"markdown","metadata":{"id":"QotMMGA1ywWQ"},"source":["### Импорт"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"5OFSe-Y6y_44"},"outputs":[],"source":["!pip install pymorphy2\n","!pip install torchmetrics"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"qKt6uizWzAXq"},"outputs":[],"source":["import json\n","import numpy as np\n","import pandas as pd\n","from zipfile import ZipFile\n","from nltk.probability import FreqDist\n","from sklearn.model_selection import train_test_split\n","\n","import torch\n","import torch.nn as nn\n","from torch.nn.utils.rnn import pad_sequence\n","import torch.nn.functional as F\n","from torch.utils.data import DataLoader, Dataset\n","from torchmetrics import F1Score\n","from torch.autograd import Variable\n","\n","import nltk\n","from nltk.tokenize import word_tokenize\n","nltk.download(\"punkt\")\n","\n","\n","random_state = 9"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"iYLTW1vGSZKJ"},"outputs":[],"source":["!mkdir ~/.kaggle\n","!touch ~/.kaggle/kaggle.json"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"j9XIaPOUQ5t1"},"outputs":[],"source":["with open('../config.json', 'r') as config_file:\n","    config = json.load(config_file)\n","TOKEN = config['TOKEN']\n","\n","api_token = {\"username\":\"w1nston\",\"key\":TOKEN}\n","\n","with open('/root/.kaggle/kaggle.json', 'w+') as file:\n","    json.dump(api_token, file)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"yTIuhvdCQ733"},"outputs":[],"source":["!chmod 600 ~/.kaggle/kaggle.json"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"HumGwcqdQ97N"},"outputs":[],"source":["!kaggle datasets download -d clmentbisaillon/fake-and-real-news-dataset"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"l_fEJVZNVury"},"outputs":[],"source":["with ZipFile('fake-and-real-news-dataset.zip', 'r') as zip_ref:\n","    zip_ref.extractall()"]},{"cell_type":"code","execution_count":244,"metadata":{"executionInfo":{"elapsed":2572,"status":"ok","timestamp":1698680044927,"user":{"displayName":"Даниил Тимофеев","userId":"04200510345473215032"},"user_tz":-180},"id":"ujI6oEEd6qFC"},"outputs":[],"source":["df_true = pd.read_csv('/content/True.csv')\n","df_fake = pd.read_csv('/content/Fake.csv')\n","\n","df_true['type'] = 0\n","df_fake['type'] = 1\n","\n","df = pd.concat([df_true, df_fake], ignore_index=True)"]},{"cell_type":"markdown","metadata":{"id":"pa_GbCG_2r_O"},"source":["### Токенизация и векторизация текстов"]},{"cell_type":"code","execution_count":245,"metadata":{"executionInfo":{"elapsed":9,"status":"ok","timestamp":1698680044928,"user":{"displayName":"Даниил Тимофеев","userId":"04200510345473215032"},"user_tz":-180},"id":"VvtEPjaG8edw"},"outputs":[],"source":["x_train_red, x_test_red, y_train, y_test = train_test_split(df['title'], df['type'],\n","                                                            shuffle=True, random_state=random_state)"]},{"cell_type":"code","execution_count":251,"metadata":{"executionInfo":{"elapsed":442,"status":"ok","timestamp":1698680232207,"user":{"displayName":"Даниил Тимофеев","userId":"04200510345473215032"},"user_tz":-180},"id":"ijscFAzh6WwV"},"outputs":[],"source":["texts = list(df['title'])"]},{"cell_type":"code","execution_count":252,"metadata":{"executionInfo":{"elapsed":9423,"status":"ok","timestamp":1698680241961,"user":{"displayName":"Даниил Тимофеев","userId":"04200510345473215032"},"user_tz":-180},"id":"J3WlATTQM3PD"},"outputs":[],"source":["tokens = [] #будет содержать все слова корпуса\n","for text in texts:\n","    tokens.extend(word_tokenize(text)) #текст -> список слов -> добавить в общий список\n","tokens_filtered = [word for word in tokens if word.isalnum()]"]},{"cell_type":"code","execution_count":253,"metadata":{"executionInfo":{"elapsed":1317,"status":"ok","timestamp":1698680302448,"user":{"displayName":"Даниил Тимофеев","userId":"04200510345473215032"},"user_tz":-180},"id":"VxdSavFnM3Uv"},"outputs":[],"source":["max_words = 1500\n","dist = FreqDist(tokens_filtered) #список слов -> словарь(слово: его частота)\n","tokens_filtered_top = [pair[0].lower() for pair in dist.most_common(max_words-1)] #возвращает топ max_words слов по частоте, саму частоту выкидывает"]},{"cell_type":"code","execution_count":254,"metadata":{"executionInfo":{"elapsed":6,"status":"ok","timestamp":1698680304048,"user":{"displayName":"Даниил Тимофеев","userId":"04200510345473215032"},"user_tz":-180},"id":"TN9-edLsM3b1"},"outputs":[],"source":["vocabulary = {v: k for k, v in dict(enumerate(tokens_filtered_top, 1)).items()}"]},{"cell_type":"code","execution_count":255,"metadata":{"executionInfo":{"elapsed":454,"status":"ok","timestamp":1698680305855,"user":{"displayName":"Даниил Тимофеев","userId":"04200510345473215032"},"user_tz":-180},"id":"ntoNwLKNM3e-"},"outputs":[],"source":["def text_to_sequence(text, maxlen):\n","    result = []\n","\n","    tokens = word_tokenize(text.lower())\n","    tokens_filtered = [word for word in tokens if word.isalnum()]\n","\n","    for word in tokens_filtered:\n","        if word in vocabulary:\n","            result.append(vocabulary[word])\n","\n","    padding = [0]*(maxlen-len(result))\n","    output = padding + result[-maxlen:]\n","\n","    return output"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"wkrMuT3GCJPs"},"outputs":[],"source":["#простая проверка для выбора длины вектора\n","len_list = []\n","for text in texts:\n","    len_list.append(len(word_tokenize(text)))\n","print(np.mean(len_list), max(len_list))"]},{"cell_type":"code","execution_count":257,"metadata":{"executionInfo":{"elapsed":11584,"status":"ok","timestamp":1698680328289,"user":{"displayName":"Даниил Тимофеев","userId":"04200510345473215032"},"user_tz":-180},"id":"wVbnaZ0mPaND"},"outputs":[],"source":["max_len = 50\n","\n","#создание векторов текстов\n","x_train = np.array([text_to_sequence(text, max_len) for text in x_train_red], dtype=np.int32)\n","x_test = np.array([text_to_sequence(text, max_len) for text in x_test_red], dtype=np.int32)\n","\n","y_test = np.array(y_test)\n","y_train = np.array(y_train)"]},{"cell_type":"markdown","metadata":{"id":"l3UVP_jg20CW"},"source":["### Обучение CNN"]},{"cell_type":"code","execution_count":265,"metadata":{"executionInfo":{"elapsed":398,"status":"ok","timestamp":1698680429536,"user":{"displayName":"Даниил Тимофеев","userId":"04200510345473215032"},"user_tz":-180},"id":"DvuPJzUQL9o_"},"outputs":[],"source":["class NewsData(Dataset):\n","    def __init__(self, data, labels):\n","        self.data = torch.from_numpy(data).long()\n","        self.labels = torch.from_numpy(labels).long()\n","\n","    def __getitem__(self, index):\n","        x = self.data[index]\n","        y = self.labels[index]\n","\n","        return x, y\n","\n","    def __len__(self):\n","        return len(self.data)"]},{"cell_type":"code","execution_count":266,"metadata":{"executionInfo":{"elapsed":395,"status":"ok","timestamp":1698680440410,"user":{"displayName":"Даниил Тимофеев","userId":"04200510345473215032"},"user_tz":-180},"id":"P-pYH2ncEpL7"},"outputs":[],"source":["class NewsClassifier(nn.Module):\n","    def __init__(self, vocab_size=1500, embedding_dim=50, out_channel=128, num_classes=2):\n","        super().__init__()\n","        self.embedding = nn.Embedding(vocab_size, embedding_dim)\n","        self.conv = nn.Conv1d(embedding_dim, out_channel, kernel_size=2)\n","        self.dropout = nn.Dropout(0.1)\n","        self.relu = nn.ReLU()\n","        self.linear = nn.Linear(out_channel, num_classes)\n","\n","\n","\n","    def forward(self, x):\n","        output = self.embedding(x)\n","        output = output.permute(0, 2, 1) # bs, emb_dim, len\n","        output = self.conv(output)\n","        output = self.relu(output)\n","        output = torch.max(output, axis=2).values\n","        output = self.dropout(output)\n","        output = self.linear(output)\n","        return output"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"lBDeP6OdUG0i"},"outputs":[],"source":["batch_size = 128\n","epochs = 7\n","\n","model = NewsClassifier()\n","print(model)\n","print(\"Parameters:\", sum([param.nelement() for param in model.parameters()]))\n","\n","model.train()\n","f1 = F1Score(task=\"binary\")\n","\n","optimizer = torch.optim.Adam(model.parameters(), lr=10e-3)\n","criterion = nn.CrossEntropyLoss()\n","\n","train_dataset = NewsData(x_train, y_train)\n","train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n","\n","loss_history = []\n","\n","for epoch in range(1,epochs+1):\n","    print(f\"Train epoch {epoch}/{epochs}\")\n","    temp_loss = []\n","    temp_metrics = []\n","    for i, (data, target) in enumerate(train_loader):\n","        optimizer.zero_grad()\n","\n","        data = data\n","        target = target\n","        output = model(data)\n","\n","        loss = criterion(output, target)\n","\n","        loss.backward()\n","\n","        optimizer.step()\n","        temp_loss.append(loss.float().item())\n","\n","        temp_metrics.append(f1(output.argmax(1), target).item())\n","\n","    epoch_loss = np.array(temp_loss).mean()\n","    epoch_f1 = np.array(temp_metrics).mean()\n","    print(f'Loss: {epoch_loss}, f1 score: {epoch_f1}')"]},{"cell_type":"markdown","metadata":{"id":"iuAN6GcYiIgU"},"source":["### Обучение LSTM"]},{"cell_type":"code","execution_count":268,"metadata":{"executionInfo":{"elapsed":352,"status":"ok","timestamp":1698680573342,"user":{"displayName":"Даниил Тимофеев","userId":"04200510345473215032"},"user_tz":-180},"id":"XA4xZ_kmjjJC"},"outputs":[],"source":["class NewsClassifierLSTM(nn.Module):\n","    def __init__(self, vocab_size=1500, embedding_dim=50, out_channel=128, n_layers=2, hidden_dim = 128, num_classes=2, drop_prob=0.5):\n","        super().__init__()\n","        self.embedding = nn.Embedding(vocab_size, embedding_dim)\n","        self.lstm = nn.LSTM(embedding_dim, out_channel, n_layers, dropout=drop_prob, batch_first=True)\n","        self.linear = nn.Linear(out_channel, num_classes)\n","        self.hidden_dim = hidden_dim\n","        self.n_layers = n_layers\n","\n","\n","    def forward(self, x):\n","        out = self.embedding(x)\n","\n","        batch_size = x.size()[0]\n","        hidden = self.init_hidden(batch_size)\n","        lstm_out, _ = self.lstm(out, hidden)\n","\n","        out = self.linear(lstm_out[:,-1])\n","\n","        return out\n","\n","    def init_hidden(self, batch_size):\n","        weight = next(self.parameters()).data\n","        hidden = (weight.new(self.n_layers, batch_size, self.hidden_dim).zero_(),\n","                  weight.new(self.n_layers, batch_size, self.hidden_dim).zero_())\n","        return hidden"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"OS61PjpljfQ2"},"outputs":[],"source":["batch_size = 128\n","epochs = 7\n","\n","model_2 = NewsClassifierLSTM()\n","print(model_2)\n","print(\"Parameters:\", sum([param.nelement() for param in model_2.parameters()]))\n","\n","model_2.train()\n","f1 = F1Score(task=\"binary\")\n","\n","optimizer = torch.optim.Adam(model_2.parameters(), lr=10e-3)\n","criterion = nn.CrossEntropyLoss()\n","\n","train_dataset = NewsData(x_train, y_train)\n","train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n","\n","loss_history = []\n","\n","for epoch in range(1,epochs+1):\n","    h = model_2.init_hidden(data.size(0))\n","    print(f\"Train epoch {epoch}/{epochs}\")\n","    temp_loss = []\n","    temp_metrics = []\n","    for i, (data, target) in enumerate(train_loader):\n","        optimizer.zero_grad()\n","\n","        data = data\n","        target = target\n","        output = model_2(data)\n","\n","        loss = criterion(output, target)\n","        loss.backward()\n","\n","        optimizer.step()\n","        temp_loss.append(loss.float().item())\n","        temp_metrics.append(f1(output.argmax(1), target).item())\n","\n","\n","    epoch_loss = np.array(temp_loss).mean()\n","    epoch_f1 = np.array(temp_metrics).mean()\n","    print(f'Loss: {epoch_loss}, f1 score: {epoch_f1}')"]},{"cell_type":"markdown","metadata":{"id":"hBrBX-zHF_GI"},"source":["### Сравнение CNN/LSTM"]},{"cell_type":"code","execution_count":280,"metadata":{"executionInfo":{"elapsed":656,"status":"ok","timestamp":1698681696869,"user":{"displayName":"Даниил Тимофеев","userId":"04200510345473215032"},"user_tz":-180},"id":"GjXtgt3lCS79"},"outputs":[],"source":["def comp(model, test_loader):\n","    temp_loss = []\n","    temp_metrics = []\n","    for i, (data, target) in enumerate(test_loader):\n","        data = data\n","        target = target\n","        output = model(data)\n","\n","        loss = criterion(output, target)\n","        temp_loss.append(loss.float().item())\n","        temp_metrics.append(f1(output.argmax(1), target).item())\n","\n","\n","    epoch_loss = np.array(temp_loss).mean()\n","    epoch_f1 = np.array(temp_metrics).mean()\n","    print(f'{epoch_loss} // {epoch_f1}')"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":70731,"status":"ok","timestamp":1698682350157,"user":{"displayName":"Даниил Тимофеев","userId":"04200510345473215032"},"user_tz":-180},"id":"2juxXuOLF7Ha","outputId":"f047f6f0-2f2c-4995-9ad4-cb7c3d1bc637"},"outputs":[],"source":["val_dataset = NewsData(x_test, y_test)\n","val_loader = DataLoader(val_dataset, batch_size=64)\n","\n","print(comp(model, val_loader))\n","print(comp(model_2, val_loader))"]}],"metadata":{"accelerator":"TPU","colab":{"authorship_tag":"ABX9TyMRitBLGp9yDjAeuU6u2WOW","provenance":[]},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.11.3"}},"nbformat":4,"nbformat_minor":0}
